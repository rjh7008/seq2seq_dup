data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971
data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(50002, 512)
    (rnn): LSTM(512, 512, num_layers=3, dropout=0.4, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(24971, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.4)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
        (2): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=24971, bias=True)
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.2)
      )
      (feed): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (remove): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU()
        (5): Dropout(p=0.2)
      )
      (mem_gate): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (softmax): Softmax()
      (selu): SELU()
      (simple): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): SELU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): Sigmoid()
      )
    )
    (dropout): Dropout(p=0.4)
  )
  (log_softmax): LogSoftmax()
  (criterion): hybrid(
    (cross): CrossEntropyLoss()
  )
)

total number of parameters: 78239627

data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971
data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(50002, 512)
    (rnn): LSTM(512, 512, num_layers=3, dropout=0.4, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(24971, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.4)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
        (2): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=24971, bias=True)
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.2)
      )
      (feed): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (remove): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU()
        (5): Dropout(p=0.2)
      )
      (mem_gate): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (softmax): Softmax()
      (selu): SELU()
      (simple): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): SELU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): Sigmoid()
      )
    )
    (dropout): Dropout(p=0.4)
  )
  (log_softmax): LogSoftmax()
  (criterion): hybrid(
    (cross): CrossEntropyLoss()
  )
)

total number of parameters: 78239627

data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971
data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(50002, 512)
    (rnn): LSTM(512, 512, num_layers=3, dropout=0.4, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(24971, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.4)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
        (2): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=24971, bias=True)
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.2)
      )
      (feed): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (remove): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU()
        (5): Dropout(p=0.2)
      )
      (mem_gate): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (softmax): Softmax()
      (selu): SELU()
      (simple): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): SELU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): Sigmoid()
      )
    )
    (dropout): Dropout(p=0.4)
  )
  (log_softmax): LogSoftmax()
  (criterion): hybrid(
    (cross): CrossEntropyLoss()
  )
)

total number of parameters: 78239627

data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971
data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	False
src_vocab_size:	50002
tgt_vocab_size:	24971

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(50002, 512)
    (rnn): LSTM(512, 512, num_layers=3, dropout=0.4, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(24971, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.4)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
        (2): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=24971, bias=True)
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.2)
      )
      (feed): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (remove): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU()
        (5): Dropout(p=0.2)
      )
      (mem_gate): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (softmax): Softmax()
      (selu): SELU()
      (simple): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): SELU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): Sigmoid()
      )
    )
    (dropout): Dropout(p=0.4)
  )
  (log_softmax): LogSoftmax()
  (criterion): hybrid(
    (cross): CrossEntropyLoss()
  )
)

total number of parameters: 78239627

data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	True
src_vocab_size:	50002
tgt_vocab_size:	24971
data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	True
src_vocab_size:	50002
tgt_vocab_size:	24971

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(50002, 512)
    (rnn): LSTM(512, 512, num_layers=3, dropout=0.4, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(24971, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.4)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
        (2): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=24971, bias=True)
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.2)
      )
      (feed): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (remove): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU()
        (5): Dropout(p=0.2)
      )
      (mem_gate): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (softmax): Softmax()
      (selu): SELU()
      (simple): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): SELU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): Sigmoid()
      )
    )
    (dropout): Dropout(p=0.4)
  )
  (log_softmax): LogSoftmax()
  (criterion): hybrid(
    (cross): CrossEntropyLoss()
  )
)

total number of parameters: 78239627

data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	True
src_vocab_size:	50002
tgt_vocab_size:	24971
data:	testdata/
logF:	log_path
epoch:	20
batch_size:	2
optim:	adam
cell:	lstm
attention:	luong_gate
learning_rate:	0.0003
max_grad_norm:	1
learning_rate_decay:	1.0
start_decay_at:	10
emb_size:	512
hidden_size:	512
dec_num_layers:	3
enc_num_layers:	3
bidirectional:	True
dropout:	0.4
max_time_step:	100
eval_interval:	25550
save_interval:	25550
metrics:	['bleu']
shared_vocab:	False
beam_size:	5
unk:	False
schedule:	True
hops:	1
schesamp:	False
res_layers:	1
resRNN:	False
selfatt:	False
attemb:	False
swish:	False
config:	en_vi.yaml
gpus:	[0]
restore:	
seed:	1234
model:	seq2seq
mode:	train
module:	seq2seq
log:	_default
num_processes:	4
refF:	
char:	False
length_norm:	False
pool_size:	0
scale:	1
max_split:	0
split_num:	0
pretrain:	
use_cuda:	True
src_vocab_size:	50002
tgt_vocab_size:	24971

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(50002, 512)
    (rnn): LSTM(512, 512, num_layers=3, dropout=0.4, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(24971, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.4)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
        (2): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=24971, bias=True)
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.2)
      )
      (feed): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (remove): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU()
        (5): Dropout(p=0.2)
      )
      (mem_gate): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU()
        (2): Dropout(p=0.2)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): Sigmoid()
        (5): Dropout(p=0.2)
      )
      (softmax): Softmax()
      (selu): SELU()
      (simple): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): SELU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): Sigmoid()
      )
    )
    (dropout): Dropout(p=0.4)
  )
  (log_softmax): LogSoftmax()
  (criterion): hybrid(
    (cross): CrossEntropyLoss()
  )
)

total number of parameters: 78239627

